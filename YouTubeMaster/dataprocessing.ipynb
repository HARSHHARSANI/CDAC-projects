{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import json\n",
    "from pyspark.sql.functions import explode, split, col\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticMDCBinder\".\n",
      "SLF4J: Defaulting to no-operation MDCAdapter implementation.\n",
      "SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|category_id|category_title|\n",
      "+-----------+--------------+\n",
      "|          1|          Film|\n",
      "|          1|     Animation|\n",
      "|          2|         Autos|\n",
      "|          2|      Vehicles|\n",
      "|         10|         Music|\n",
      "|         15|          Pets|\n",
      "|         15|       Animals|\n",
      "|         17|        Sports|\n",
      "|         19|        Travel|\n",
      "|         19|        Events|\n",
      "|         20|        Gaming|\n",
      "|         22|        People|\n",
      "|         22|         Blogs|\n",
      "|         23|        Comedy|\n",
      "|         24| Entertainment|\n",
      "|         25|          News|\n",
      "|         25|      Politics|\n",
      "|         26|         Howto|\n",
      "|         26|         Style|\n",
      "|         27|     Education|\n",
      "+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ./youtube_categories.json\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YouTubeDataCleaning\").getOrCreate()\n",
    "\n",
    "data_dir = \"./data\"\n",
    "\n",
    "json_files = [f for f in os.listdir(data_dir) if f.endswith(\".json\")]\n",
    "\n",
    "dfs = []\n",
    "\n",
    "# Loop through each JSON file\n",
    "for json_file in json_files:\n",
    "    file_path = os.path.join(data_dir, json_file)\n",
    "\n",
    "    # Load JSON Data\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:  # Open in text mode for encoding\n",
    "            data = json.load(f)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON in file {json_file}: {e}\")\n",
    "        continue  # Skip to the next file on error\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    # Extract only 'items' array\n",
    "    items = data.get(\"items\", [])\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = spark.createDataFrame(items)\n",
    "\n",
    "\n",
    "    df.head(10)\n",
    "\n",
    "    # Select relevant fields\n",
    "    df_cleaned = df.select(\n",
    "        col(\"id\").alias(\"category_id\"),\n",
    "        col(\"snippet.title\").alias(\"category_title\")\n",
    "    ).where(col(\"snippet.assignable\") == True)\n",
    "\n",
    "    # Split category_title into multiple words and explode\n",
    "    df_exploded = df_cleaned.withColumn(\"category_title\", explode(split(col(\"category_title\"), \" \")))\n",
    "\n",
    "    # Remove rows where category_title is '&'\n",
    "    df_exploded = df_exploded.filter(col(\"category_title\") != \"&\")\n",
    "\n",
    "    # Append the processed DataFrame to the list\n",
    "    dfs.append(df_exploded)\n",
    "\n",
    "# Union all DataFrames\n",
    "if dfs:\n",
    "    final_df = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        final_df = final_df.union(df)\n",
    "\n",
    "    # Show the final DataFrame (optional)\n",
    "    final_df.show()\n",
    "    final_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"./youtube_categories.json\")\n",
    "    print(\"Data saved to ./youtube_categories.json\")\n",
    "\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"No JSON files processed.\")\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
