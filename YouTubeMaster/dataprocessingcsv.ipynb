{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticMDCBinder\".\n",
      "SLF4J: Defaulting to no-operation MDCAdapter implementation.\n",
      "SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.\n",
      "                                                                                \r"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[INCOMPATIBLE_COLUMN_TYPE] UNION can only be performed on tables with compatible column types. The 8th column of the second table is \"STRING\" type which is not compatible with \"BOOLEAN\" at the same column of the first table..;\n'Union false, false\n:- Project [trending_date#18, category_id#21, publish_time#22, views#24, likes#25, dislikes#26, cast(comment_count#27 as string) AS comment_count#164, comments_disabled#29, ratings_disabled#30, video_error_or_removed#31, tag#53]\n:  +- Project [trending_date#18, category_id#21, publish_time#22, views#24, likes#25, dislikes#26, comment_count#27, comments_disabled#29, ratings_disabled#30, video_error_or_removed#31, tag#53]\n:     +- Project [video_id#17, trending_date#18, title#19, channel_title#20, category_id#21, publish_time#22, tags#23, views#24, likes#25, dislikes#26, comment_count#27, thumbnail_link#28, comments_disabled#29, ratings_disabled#30, video_error_or_removed#31, description#32, tag#53]\n:        +- Generate explode(split(tags#23, \\|, -1)), false, [tag#53]\n:           +- Filter NOT is_mostly_empty(struct(video_id, video_id#17, trending_date, trending_date#18, title, title#19, channel_title, channel_title#20, category_id, category_id#21, publish_time, publish_time#22, tags, tags#23, views, views#24, likes, likes#25, dislikes, dislikes#26, comment_count, comment_count#27, thumbnail_link, thumbnail_link#28, ... 8 more fields))#51\n:              +- Filter is_mostly_english(tags#23)#49\n:                 +- Relation [video_id#17,trending_date#18,title#19,channel_title#20,category_id#21,publish_time#22,tags#23,views#24,likes#25,dislikes#26,comment_count#27,thumbnail_link#28,comments_disabled#29,ratings_disabled#30,video_error_or_removed#31,description#32] csv\n+- Project [trending_date#100, category_id#103, publish_time#104, views#106, likes#107, dislikes#108, comment_count#109, comments_disabled#111, ratings_disabled#112, video_error_or_removed#113, tag#135]\n   +- Project [video_id#99, trending_date#100, title#101, channel_title#102, category_id#103, publish_time#104, tags#105, views#106, likes#107, dislikes#108, comment_count#109, thumbnail_link#110, comments_disabled#111, ratings_disabled#112, video_error_or_removed#113, description#114, tag#135]\n      +- Generate explode(split(tags#105, \\|, -1)), false, [tag#135]\n         +- Filter NOT is_mostly_empty(struct(video_id, video_id#99, trending_date, trending_date#100, title, title#101, channel_title, channel_title#102, category_id, category_id#103, publish_time, publish_time#104, tags, tags#105, views, views#106, likes, likes#107, dislikes, dislikes#108, comment_count, comment_count#109, thumbnail_link, thumbnail_link#110, ... 8 more fields))#133\n            +- Filter is_mostly_english(tags#105)#131\n               +- Relation [video_id#99,trending_date#100,title#101,channel_title#102,category_id#103,publish_time#104,tags#105,views#106,likes#107,dislikes#108,comment_count#109,thumbnail_link#110,comments_disabled#111,ratings_disabled#112,video_error_or_removed#113,description#114] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 67\u001b[0m\n\u001b[1;32m     65\u001b[0m         all_data \u001b[38;5;241m=\u001b[39m df\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m         all_data \u001b[38;5;241m=\u001b[39m \u001b[43mall_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Use DataFrame `union()` instead of SQL `UNION`\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# If no data was processed, print a message and exit\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m all_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:3929\u001b[0m, in \u001b[0;36mDataFrame.union\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   3833\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munion\u001b[39m(\u001b[38;5;28mself\u001b[39m, other: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3834\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a new :class:`DataFrame` containing the union of rows in this and another\u001b[39;00m\n\u001b[1;32m   3835\u001b[0m \u001b[38;5;124;03m    :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   3836\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3927\u001b[0m \u001b[38;5;124;03m    +---+-----+\u001b[39;00m\n\u001b[1;32m   3928\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3929\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [INCOMPATIBLE_COLUMN_TYPE] UNION can only be performed on tables with compatible column types. The 8th column of the second table is \"STRING\" type which is not compatible with \"BOOLEAN\" at the same column of the first table..;\n'Union false, false\n:- Project [trending_date#18, category_id#21, publish_time#22, views#24, likes#25, dislikes#26, cast(comment_count#27 as string) AS comment_count#164, comments_disabled#29, ratings_disabled#30, video_error_or_removed#31, tag#53]\n:  +- Project [trending_date#18, category_id#21, publish_time#22, views#24, likes#25, dislikes#26, comment_count#27, comments_disabled#29, ratings_disabled#30, video_error_or_removed#31, tag#53]\n:     +- Project [video_id#17, trending_date#18, title#19, channel_title#20, category_id#21, publish_time#22, tags#23, views#24, likes#25, dislikes#26, comment_count#27, thumbnail_link#28, comments_disabled#29, ratings_disabled#30, video_error_or_removed#31, description#32, tag#53]\n:        +- Generate explode(split(tags#23, \\|, -1)), false, [tag#53]\n:           +- Filter NOT is_mostly_empty(struct(video_id, video_id#17, trending_date, trending_date#18, title, title#19, channel_title, channel_title#20, category_id, category_id#21, publish_time, publish_time#22, tags, tags#23, views, views#24, likes, likes#25, dislikes, dislikes#26, comment_count, comment_count#27, thumbnail_link, thumbnail_link#28, ... 8 more fields))#51\n:              +- Filter is_mostly_english(tags#23)#49\n:                 +- Relation [video_id#17,trending_date#18,title#19,channel_title#20,category_id#21,publish_time#22,tags#23,views#24,likes#25,dislikes#26,comment_count#27,thumbnail_link#28,comments_disabled#29,ratings_disabled#30,video_error_or_removed#31,description#32] csv\n+- Project [trending_date#100, category_id#103, publish_time#104, views#106, likes#107, dislikes#108, comment_count#109, comments_disabled#111, ratings_disabled#112, video_error_or_removed#113, tag#135]\n   +- Project [video_id#99, trending_date#100, title#101, channel_title#102, category_id#103, publish_time#104, tags#105, views#106, likes#107, dislikes#108, comment_count#109, thumbnail_link#110, comments_disabled#111, ratings_disabled#112, video_error_or_removed#113, description#114, tag#135]\n      +- Generate explode(split(tags#105, \\|, -1)), false, [tag#135]\n         +- Filter NOT is_mostly_empty(struct(video_id, video_id#99, trending_date, trending_date#100, title, title#101, channel_title, channel_title#102, category_id, category_id#103, publish_time, publish_time#104, tags, tags#105, views, views#106, likes, likes#107, dislikes, dislikes#108, comment_count, comment_count#109, thumbnail_link, thumbnail_link#110, ... 8 more fields))#133\n            +- Filter is_mostly_english(tags#105)#131\n               +- Relation [video_id#99,trending_date#100,title#101,channel_title#102,category_id#103,publish_time#104,tags#105,views#106,likes#107,dislikes#108,comment_count#109,thumbnail_link#110,comments_disabled#111,ratings_disabled#112,video_error_or_removed#113,description#114] csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, split, udf, struct\n",
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YouTubeDataCleaning\").getOrCreate()\n",
    "\n",
    "# Directory containing the CSV files\n",
    "data_dir = \"./data\"\n",
    "\n",
    "# Get a list of all CSV files in the directory\n",
    "csv_files = [f for f in os.listdir(data_dir) if f.endswith(\".csv\")]\n",
    "\n",
    "# Function to check if a string is mostly English\n",
    "def is_mostly_english(text):\n",
    "    if not text:  # Handle empty strings\n",
    "        return False\n",
    "    try:\n",
    "        text.encode('utf-8').decode('ascii')\n",
    "        return True\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "\n",
    "# Register the UDF\n",
    "is_mostly_english_udf = udf(is_mostly_english, BooleanType())\n",
    "\n",
    "# Function to check for rows with mostly empty fields\n",
    "def is_mostly_empty(row):\n",
    "    non_empty_count = sum(1 for value in row if value)\n",
    "    return non_empty_count < 3  # Adjust threshold as needed\n",
    "\n",
    "# Initialize an empty DataFrame for `all_data`\n",
    "all_data = None\n",
    "\n",
    "# Loop through each CSV file\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(data_dir, csv_file)\n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "    try:\n",
    "        df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {csv_file}: {e}\")\n",
    "        continue  # Skip to the next file if there's an error\n",
    "\n",
    "    # Remove non-English rows (Apply the UDF to relevant columns)\n",
    "    if \"tags\" in df.columns:\n",
    "        df = df.filter(is_mostly_english_udf(col(\"tags\")))\n",
    "\n",
    "    # Remove mostly empty rows\n",
    "    row_values = [df[field] for field in df.columns]\n",
    "    df = df.filter(~udf(is_mostly_empty, BooleanType())(struct(*row_values)))\n",
    "\n",
    "    # Explode the tags column\n",
    "    if \"tags\" in df.columns:\n",
    "        df = df.withColumn(\"tag\", explode(split(col(\"tags\"), r\"\\|\")))\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    columns_to_drop = ['video_id', 'thumbnail_link', 'description', 'title', 'channel_title', 'tags']\n",
    "    df = df.drop(*[col_name for col_name in columns_to_drop if col_name in df.columns])\n",
    "\n",
    "    # Merge into `all_data`\n",
    "    if all_data is None:\n",
    "        all_data = df\n",
    "    else:\n",
    "        all_data = all_data.union(df)  # Use DataFrame `union()` instead of SQL `UNION`\n",
    "\n",
    "# If no data was processed, print a message and exit\n",
    "if all_data is None:\n",
    "    print(\"No CSV files were processed.\")\n",
    "else:\n",
    "    all_data.show()\n",
    "    # Save the final DataFrame to a single CSV file\n",
    "    all_data.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"cleaned_youtube_data.csv\")\n",
    "    print(\"Data saved to cleaned_youtube_data.csv\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
