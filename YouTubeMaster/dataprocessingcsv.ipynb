{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticMDCBinder\".\n",
      "SLF4J: Defaulting to no-operation MDCAdapter implementation.\n",
      "SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+--------------------+--------+------+--------+-------------+-----------------+----------------+----------------------+--------------------+\n",
      "|trending_date|category_id|        publish_time|   views| likes|dislikes|comment_count|comments_disabled|ratings_disabled|video_error_or_removed|                 tag|\n",
      "+-------------+-----------+--------------------+--------+------+--------+-------------+-----------------+----------------+----------------------+--------------------+\n",
      "|     17.14.11|         10|2017-11-10T17:00:...|17158579|787425|   43420|       125882|            False|           False|                 False|            \"Eminem\"|\n",
      "|     17.14.11|         10|2017-11-10T17:00:...|17158579|787425|   43420|       125882|            False|           False|                 False|              \"Walk\"|\n",
      "|     17.14.11|         10|2017-11-10T17:00:...|17158579|787425|   43420|       125882|            False|           False|                 False|                \"On\"|\n",
      "|     17.14.11|         10|2017-11-10T17:00:...|17158579|787425|   43420|       125882|            False|           False|                 False|             \"Water\"|\n",
      "|     17.14.11|         10|2017-11-10T17:00:...|17158579|787425|   43420|       125882|            False|           False|                 False|\"Aftermath/Shady/...|\n",
      "|     17.14.11|         10|2017-11-10T17:00:...|17158579|787425|   43420|       125882|            False|           False|                 False|               \"Rap\"|\n",
      "|     17.14.11|         23|2017-11-13T17:00:...| 1014651|127794|    1688|        13030|            False|           False|                 False|             \"plush\"|\n",
      "|     17.14.11|         23|2017-11-13T17:00:...| 1014651|127794|    1688|        13030|            False|           False|                 False|      \"bad unboxing\"|\n",
      "|     17.14.11|         23|2017-11-13T17:00:...| 1014651|127794|    1688|        13030|            False|           False|                 False|          \"unboxing\"|\n",
      "|     17.14.11|         23|2017-11-13T17:00:...| 1014651|127794|    1688|        13030|            False|           False|                 False|          \"fan mail\"|\n",
      "|     17.14.11|         23|2017-11-13T17:00:...| 1014651|127794|    1688|        13030|            False|           False|                 False|         \"idubbbztv\"|\n",
      "|     17.14.11|         23|2017-11-13T17:00:...| 1014651|127794|    1688|        13030|            False|           False|                 False|        \"idubbbztv2\"|\n",
      "|     17.14.11|         23|2017-11-13T17:00:...| 1014651|127794|    1688|        13030|            False|           False|                 False|            \"things\"|\n",
      "|     17.14.11|         23|2017-11-13T17:00:...| 1014651|127794|    1688|        13030|            False|           False|                 False|              \"best\"|\n",
      "|     17.14.11|         23|2017-11-13T17:00:...| 1014651|127794|    1688|        13030|            False|           False|                 False|          \"packages\"|\n",
      "|     17.14.11|         23|2017-11-13T17:00:...| 1014651|127794|    1688|        13030|            False|           False|                 False|          \"plushies\"|\n",
      "|     17.14.11|         23|2017-11-13T17:00:...| 1014651|127794|    1688|        13030|            False|           False|                 False|     \"chontent chop\"|\n",
      "|     17.14.11|         23|2017-11-12T19:05:...| 3191434|146035|    5339|         8181|            False|           False|                 False|   \"racist superman\"|\n",
      "|     17.14.11|         23|2017-11-12T19:05:...| 3191434|146035|    5339|         8181|            False|           False|                 False|              \"rudy\"|\n",
      "|     17.14.11|         23|2017-11-12T19:05:...| 3191434|146035|    5339|         8181|            False|           False|                 False|           \"mancuso\"|\n",
      "+-------------+-----------+--------------------+--------+------+--------+-------------+-----------------+----------------+----------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to cleaned_youtube_data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, split, udf, struct\n",
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YouTubeDataCleaning\").getOrCreate()\n",
    "\n",
    "# Directory containing the CSV files\n",
    "data_dir = \"./data\"\n",
    "\n",
    "# Get a list of all CSV files in the directory\n",
    "csv_files = [f for f in os.listdir(data_dir) if f.endswith(\".csv\")]\n",
    "\n",
    "# Function to check if a string is mostly English\n",
    "def is_mostly_english(text):\n",
    "    if not text:  # Handle empty strings\n",
    "        return False\n",
    "    try:\n",
    "        text.encode('utf-8').decode('ascii')\n",
    "        return True\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "\n",
    "# Register the UDF\n",
    "is_mostly_english_udf = udf(is_mostly_english, BooleanType())\n",
    "\n",
    "# Function to check for rows with mostly empty fields\n",
    "def is_mostly_empty(row):\n",
    "    non_empty_count = 0\n",
    "    for value in row:\n",
    "        if value:\n",
    "            non_empty_count += 1\n",
    "    return non_empty_count < 3  # Adjust threshold as needed\n",
    "\n",
    "# Loop through each CSV file\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(data_dir, csv_file)\n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "    try:\n",
    "        df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {csv_file}: {e}\")\n",
    "        continue  # Skip to the next file if there's an error\n",
    "\n",
    "    # Remove non-English rows (Apply the UDF to relevant columns)\n",
    "    columns_to_check = ['tags']  # Only check 'tags' for English\n",
    "    for column in columns_to_check:\n",
    "        if column in df.columns:\n",
    "            df = df.filter(is_mostly_english_udf(col(column)))\n",
    "\n",
    "    # Remove mostly empty rows\n",
    "    row_values = [df[field] for field in df.columns]\n",
    "    df = df.filter(~udf(lambda row: is_mostly_empty(row), BooleanType())(struct(*row_values)))\n",
    "\n",
    "    # Explode the tags column\n",
    "    df = df.withColumn(\"tag\", explode(split(col(\"tags\"), \"\\|\")))\n",
    "\n",
    "    # Columns to drop\n",
    "    columns_to_drop = ['video_id', 'thumbnail_link', 'description', 'title', 'channel_title', 'tags']\n",
    "    columns_to_drop = [col_name for col_name in columns_to_drop if col_name in df.columns]\n",
    "    df = df.drop(*columns_to_drop)\n",
    "\n",
    "    # Register the processed dataframe\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "\n",
    "    if 'all_data' in locals():\n",
    "        all_data = spark.sql(\"SELECT * FROM all_data UNION ALL SELECT * FROM temp_table\")\n",
    "    else:\n",
    "        all_data = spark.sql(\"SELECT * FROM temp_table\")\n",
    "\n",
    "# Remove the temporary table\n",
    "spark.catalog.dropTempView(\"temp_table\")\n",
    "\n",
    "if 'all_data' in locals():\n",
    "    all_data.show()\n",
    "    # Save the final DataFrame to a single CSV file\n",
    "    all_data.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\n",
    "        \"cleaned_youtube_data.csv\")\n",
    "    print(\"Data saved to cleaned_youtube_data.csv\")\n",
    "else:\n",
    "    print(\"No CSV files were processed.\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
