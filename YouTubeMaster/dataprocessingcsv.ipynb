{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticMDCBinder\".\n",
      "SLF4J: Defaulting to no-operation MDCAdapter implementation.\n",
      "SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+------------------------+-------+------+--------+-------------+-----------------+----------------+----------------------+---+\n",
      "|trending_date|category_id|publish_time            |views  |likes |dislikes|comment_count|comments_disabled|ratings_disabled|video_error_or_removed|tag|\n",
      "+-------------+-----------+------------------------+-------+------+--------+-------------+-----------------+----------------+----------------------+---+\n",
      "|18.07.02     |1          |2018-02-06T04:01:56.000Z|90929  |442   |88      |174          |false            |false           |false                 |[  |\n",
      "|18.07.02     |1          |2018-02-06T04:01:56.000Z|90929  |442   |88      |174          |false            |false           |false                 |n  |\n",
      "|18.07.02     |1          |2018-02-06T04:01:56.000Z|90929  |442   |88      |174          |false            |false           |false                 |o  |\n",
      "|18.07.02     |1          |2018-02-06T04:01:56.000Z|90929  |442   |88      |174          |false            |false           |false                 |n  |\n",
      "|18.07.02     |1          |2018-02-06T04:01:56.000Z|90929  |442   |88      |174          |false            |false           |false                 |e  |\n",
      "|18.07.02     |1          |2018-02-06T04:01:56.000Z|90929  |442   |88      |174          |false            |false           |false                 |]  |\n",
      "|18.07.02     |1          |2018-02-06T04:01:56.000Z|90929  |442   |88      |174          |false            |false           |false                 |   |\n",
      "|18.07.02     |28         |2018-02-06T21:38:22.000Z|6408303|165892|2331    |3006         |false            |false           |false                 |[  |\n",
      "|18.07.02     |28         |2018-02-06T21:38:22.000Z|6408303|165892|2331    |3006         |false            |false           |false                 |n  |\n",
      "|18.07.02     |28         |2018-02-06T21:38:22.000Z|6408303|165892|2331    |3006         |false            |false           |false                 |o  |\n",
      "|18.07.02     |28         |2018-02-06T21:38:22.000Z|6408303|165892|2331    |3006         |false            |false           |false                 |n  |\n",
      "|18.07.02     |28         |2018-02-06T21:38:22.000Z|6408303|165892|2331    |3006         |false            |false           |false                 |e  |\n",
      "|18.07.02     |28         |2018-02-06T21:38:22.000Z|6408303|165892|2331    |3006         |false            |false           |false                 |]  |\n",
      "|18.07.02     |28         |2018-02-06T21:38:22.000Z|6408303|165892|2331    |3006         |false            |false           |false                 |   |\n",
      "|18.07.02     |1          |2018-02-06T02:30:00.000Z|108408 |1336  |74      |201          |false            |false           |false                 |[  |\n",
      "|18.07.02     |1          |2018-02-06T02:30:00.000Z|108408 |1336  |74      |201          |false            |false           |false                 |n  |\n",
      "|18.07.02     |1          |2018-02-06T02:30:00.000Z|108408 |1336  |74      |201          |false            |false           |false                 |o  |\n",
      "|18.07.02     |1          |2018-02-06T02:30:00.000Z|108408 |1336  |74      |201          |false            |false           |false                 |n  |\n",
      "|18.07.02     |1          |2018-02-06T02:30:00.000Z|108408 |1336  |74      |201          |false            |false           |false                 |e  |\n",
      "|18.07.02     |1          |2018-02-06T02:30:00.000Z|108408 |1336  |74      |201          |false            |false           |false                 |]  |\n",
      "+-------------+-----------+------------------------+-------+------+--------+-------------+-----------------+----------------+----------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to cleaned_youtube_data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, split, udf, struct\n",
    "from pyspark.sql.types import BooleanType, IntegerType, StringType\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"YouTubeDataCleaning\").getOrCreate()\n",
    "\n",
    "# Directory containing the CSV files\n",
    "data_dir = \"./data\"\n",
    "\n",
    "# Get a list of all CSV files in the directory\n",
    "csv_files = [f for f in os.listdir(data_dir) if f.endswith(\".csv\")]\n",
    "\n",
    "# Function to check if a string is mostly English\n",
    "def is_mostly_english(text):\n",
    "    if not text:  # Handle empty strings\n",
    "        return False\n",
    "    try:\n",
    "        text.encode('utf-8').decode('ascii')\n",
    "        return True\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "\n",
    "# Register the UDF\n",
    "is_mostly_english_udf = udf(is_mostly_english, BooleanType())\n",
    "\n",
    "# Function to check for rows with mostly empty fields\n",
    "def is_mostly_empty(row):\n",
    "    non_empty_count = sum(1 for value in row if value)\n",
    "    return non_empty_count < 3  # Adjust threshold as needed\n",
    "\n",
    "# Initialize an empty DataFrame for `all_data`\n",
    "all_data = None\n",
    "\n",
    "# Loop through each CSV file\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(data_dir, csv_file)\n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "    try:\n",
    "        df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {csv_file}: {e}\")\n",
    "        continue  # Skip to the next file if there's an error\n",
    "\n",
    "    # Remove non-English rows (Apply the UDF to relevant columns)\n",
    "    if \"tags\" in df.columns:\n",
    "        df = df.filter(is_mostly_english_udf(col(\"tags\")))\n",
    "\n",
    "    # Remove mostly empty rows\n",
    "    row_values = [df[field] for field in df.columns]\n",
    "    df = df.filter(~udf(is_mostly_empty, BooleanType())(struct(*row_values)))\n",
    "\n",
    "    # Explode the tags column\n",
    "    if \"tags\" in df.columns:\n",
    "        df = df.withColumn(\"tag\", explode(split(col(\"tags\"), r\"\\\\|\")))\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    columns_to_drop = ['video_id', 'thumbnail_link', 'description', 'title', 'channel_title', 'tags']\n",
    "    df = df.drop(*[col_name for col_name in columns_to_drop if col_name in df.columns])\n",
    "\n",
    "    # Ensure consistent column data types\n",
    "    df = df.withColumn(\"comment_count\", col(\"comment_count\").cast(IntegerType()))\n",
    "    df = df.withColumn(\"comments_disabled\", col(\"comments_disabled\").cast(BooleanType()))\n",
    "    df = df.withColumn(\"ratings_disabled\", col(\"ratings_disabled\").cast(BooleanType()))\n",
    "    df = df.withColumn(\"video_error_or_removed\", col(\"video_error_or_removed\").cast(BooleanType()))\n",
    "\n",
    "    # Merge into `all_data`\n",
    "    if all_data is None:\n",
    "        all_data = df\n",
    "    else:\n",
    "        all_data = all_data.unionByName(df, allowMissingColumns=True)  # Ensure matching column names\n",
    "\n",
    "# If no data was processed, print a message and exit\n",
    "if all_data is None:\n",
    "    print(\"No CSV files were processed.\")\n",
    "else:\n",
    "    all_data.show(truncate=False)\n",
    "    # Save the final DataFrame to a single CSV file\n",
    "    all_data.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"cleaned_youtube_data.csv\")\n",
    "    print(\"Data saved to cleaned_youtube_data.csv\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
